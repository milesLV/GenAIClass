{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (2.6.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (1.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from openai) (2.12.3)\n",
      "Requirement already satisfied: sniffio in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/miles5/Desktop/Work/GithubRepos/GenAIClass/genAI_Lab/.venv/lib/python3.14/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf87443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766e477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-pr\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True, dotenv_path=\"../.env\") # make sure to save the .env file before you load it in\n",
    "myAPIKey = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(myAPIKey[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "614b0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=myAPIKey)\n",
    "model = \"gpt-5-nano\"  # best for our purposes\n",
    "def askChatBotQuestion(prompt):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer as correctly as possible.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    print(response)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a63fb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Prompt: Why is Docker useful for data scientists and data engineers?\n",
      "ChatCompletion(id='chatcmpl-CUidLDEXHAHD1520guDWstsiEDgA1', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Docker is useful for both data scientists and data engineers for several overlapping reasons. Here’s a concise guide to why and how it helps.\\n\\nKey benefits\\n\\n- Reproducibility and isolation\\n  - Ensures the exact same software stack (Python/R versions, libraries, OS libs) across machines and over time.\\n  - Prevents “it works on my machine” issues caused by dependency drift.\\n\\n- Dependency and environment management\\n  - Encapsulates all dependencies in a single image or set of images.\\n  - Makes pinning versions explicit and portable, including system packages, not just Python packages.\\n\\n- Portability and consistency\\n  - A container runs the same way on a laptop, workstation, server, or cloud, regardless of the host OS.\\n  - Facilitates collaboration: teammates run identical environments with minimal setup.\\n\\n- Collaboration and sharing\\n  - Images can be shared via registries (public or private); experiments, notebooks, and pipelines can be versioned and reused.\\n  - Helps standardize workflows (e.g., notebooks, data processing scripts, model training) across teams.\\n\\n- Data pipelines and orchestration\\n  - Data engineers can containerize ETL jobs, Airflow tasks, Spark jobs, or Kedro pipelines, ensuring consistent runtimes.\\n  - Simplifies deployment of services like databases, message queues, or storage adapters alongside processing code.\\n\\n- Ease of experimentation and notebooks\\n  - Launch a Jupyter (or VS Code) environment with all required libs and data mounted, enabling quick, reproducible experimentation.\\n  - You can run multiple experiments in parallel with isolated environments.\\n\\n- Deployability to prod and cloud\\n  - Same image can be used for development, testing, staging, and production.\\n  - Integrates with orchestration systems (Kubernetes, Docker Swarm) for scaling, scheduling, and fault tolerance.\\n\\n- GPU and specialized hardware support\\n  - With NVIDIA Docker/Container Toolkit, you can run GPU-enabled ML workloads with proper driver libraries inside containers.\\n  - Keeps host drivers separate from the container’s runtime.\\n\\n- Resource control and stability\\n  - Containers can limit CPU/memory usage for fair sharing and stable pipelines.\\n  - Easy to transform ad-hoc scripts into robust, failed-safe jobs.\\n\\nCommon patterns for data scientists\\n\\n- Notebook-first workflows\\n  - Use a container image that includes Python/R data stacks and a Jupyter server; mount notebooks and data as volumes.\\n  - Example use: a Jupyter environment with numpy, pandas, scikit-learn, seaborn, pytorch/tensorflow, and GPU support if needed.\\n\\n- Reproducible experiments\\n  - Run experiments inside containers to ensure the same code, data access patterns, and libraries, making it easier to reproduce results later.\\n\\n- Lightweight, portable environments\\n  - Build small, layered images (base image + dependencies) to minimize rebuild time and disk usage.\\n\\nCommon patterns for data engineers\\n\\n- Containerized ETL and orchestration\\n  - Containerize individual ETL steps (or entire DAG tasks) and run them with a scheduler like Airflow, Kedro, or Prefect.\\n  - Use containers for databases, message queues, and storage services when appropriate (or connect to managed services).\\n\\n- Data and model serving\\n  - Package serving code (APIs for models) in containers to ensure consistency between development and production.\\n\\n- CI/CD for data work\\n  - Use containerized tests for data pipelines, model validation, and integration tests as part of a CI pipeline.\\n\\nStarter ideas (what to start with)\\n\\n- Data science workstation\\n  - A minimal Python/R stack in a container, Jupyter server exposed on a port, a mounted local repo, and a mounted data directory.\\n\\n- Simple ETL service\\n  - A container running a small Python or Spark job that reads from a source, processes data, and writes to a destination, orchestrated by a workflow tool.\\n\\n- Model training service\\n  - A container that trains a model on a dataset, logs metrics, and stores artifacts to a model registry or storage.\\n\\nBest practices\\n\\n- Keep images lean\\n  - Use slim/base images, pin versions, and remove build-time secrets. Use multi-stage builds when you need build tools only for compilation.\\n\\n- Security and access\\n  - Run processes as non-root where possible; avoid embedding credentials in images; use env vars or secret managers.\\n\\n- Reproducibility and dependencies\\n  - Use a requirements.txt (or poetry) and system package lists that are well-scoped and pinned.\\n\\n- Versioning and documentation\\n  - Tag images clearly (e.g., project-name:experiment-2025-06-01) and maintain a Dockerfile and a brief README describing the environment.\\n\\n- Data handling\\n  - Use volumes or bind mounts for data to keep datasets external to the container and easily updatable.\\n  - Be mindful of data provenance and lineage when containerizing data transformations.\\n\\nCommon pitfalls to avoid\\n\\n- Large, bloated images\\n  - Regularly prune unused layers and data; prefer multi-stage builds.\\n\\n- Data drift and hidden dependencies\\n  - If data or external services change, rebuild and re-test images to catch drift early.\\n\\n- Over-reliance on container orchestration\\n  - Start simple: containers on a single machine with docker-compose before moving to Kubernetes.\\n\\n- Not handling secrets securely\\n  - Do not bake credentials into images; use environment variables, secret stores, or secret management in the cluster.\\n\\nQuick starter tips\\n\\n- Try official images for data science work (e.g., Jupyter stacks) as a quick start.\\n- Use volumes for code and data to preserve work outside containers.\\n- Use docker-compose to define related services (e.g., notebook, database, and a data service) in one go.\\n- For GPU workloads, ensure the NVIDIA container toolkit is installed and the runtime is set to nvidia.\\n\\nIf you’d like, tell me your use case (e.g., notebook-based exploration, ETL pipelines, model training, or deployment), and I can propose a concrete Dockerfile and a simple docker-compose setup tailored to your stack.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1761437447, model='gpt-5-nano-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=2333, prompt_tokens=35, total_tokens=2368, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=1088, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "gpt-5-nano says: Docker is useful for both data scientists and data engineers for several overlapping reasons. Here’s a concise guide to why and how it helps.\n",
      "\n",
      "Key benefits\n",
      "\n",
      "- Reproducibility and isolation\n",
      "  - Ensures the exact same software stack (Python/R versions, libraries, OS libs) across machines and over time.\n",
      "  - Prevents “it works on my machine” issues caused by dependency drift.\n",
      "\n",
      "- Dependency and environment management\n",
      "  - Encapsulates all dependencies in a single image or set of images.\n",
      "  - Makes pinning versions explicit and portable, including system packages, not just Python packages.\n",
      "\n",
      "- Portability and consistency\n",
      "  - A container runs the same way on a laptop, workstation, server, or cloud, regardless of the host OS.\n",
      "  - Facilitates collaboration: teammates run identical environments with minimal setup.\n",
      "\n",
      "- Collaboration and sharing\n",
      "  - Images can be shared via registries (public or private); experiments, notebooks, and pipelines can be versioned and reused.\n",
      "  - Helps standardize workflows (e.g., notebooks, data processing scripts, model training) across teams.\n",
      "\n",
      "- Data pipelines and orchestration\n",
      "  - Data engineers can containerize ETL jobs, Airflow tasks, Spark jobs, or Kedro pipelines, ensuring consistent runtimes.\n",
      "  - Simplifies deployment of services like databases, message queues, or storage adapters alongside processing code.\n",
      "\n",
      "- Ease of experimentation and notebooks\n",
      "  - Launch a Jupyter (or VS Code) environment with all required libs and data mounted, enabling quick, reproducible experimentation.\n",
      "  - You can run multiple experiments in parallel with isolated environments.\n",
      "\n",
      "- Deployability to prod and cloud\n",
      "  - Same image can be used for development, testing, staging, and production.\n",
      "  - Integrates with orchestration systems (Kubernetes, Docker Swarm) for scaling, scheduling, and fault tolerance.\n",
      "\n",
      "- GPU and specialized hardware support\n",
      "  - With NVIDIA Docker/Container Toolkit, you can run GPU-enabled ML workloads with proper driver libraries inside containers.\n",
      "  - Keeps host drivers separate from the container’s runtime.\n",
      "\n",
      "- Resource control and stability\n",
      "  - Containers can limit CPU/memory usage for fair sharing and stable pipelines.\n",
      "  - Easy to transform ad-hoc scripts into robust, failed-safe jobs.\n",
      "\n",
      "Common patterns for data scientists\n",
      "\n",
      "- Notebook-first workflows\n",
      "  - Use a container image that includes Python/R data stacks and a Jupyter server; mount notebooks and data as volumes.\n",
      "  - Example use: a Jupyter environment with numpy, pandas, scikit-learn, seaborn, pytorch/tensorflow, and GPU support if needed.\n",
      "\n",
      "- Reproducible experiments\n",
      "  - Run experiments inside containers to ensure the same code, data access patterns, and libraries, making it easier to reproduce results later.\n",
      "\n",
      "- Lightweight, portable environments\n",
      "  - Build small, layered images (base image + dependencies) to minimize rebuild time and disk usage.\n",
      "\n",
      "Common patterns for data engineers\n",
      "\n",
      "- Containerized ETL and orchestration\n",
      "  - Containerize individual ETL steps (or entire DAG tasks) and run them with a scheduler like Airflow, Kedro, or Prefect.\n",
      "  - Use containers for databases, message queues, and storage services when appropriate (or connect to managed services).\n",
      "\n",
      "- Data and model serving\n",
      "  - Package serving code (APIs for models) in containers to ensure consistency between development and production.\n",
      "\n",
      "- CI/CD for data work\n",
      "  - Use containerized tests for data pipelines, model validation, and integration tests as part of a CI pipeline.\n",
      "\n",
      "Starter ideas (what to start with)\n",
      "\n",
      "- Data science workstation\n",
      "  - A minimal Python/R stack in a container, Jupyter server exposed on a port, a mounted local repo, and a mounted data directory.\n",
      "\n",
      "- Simple ETL service\n",
      "  - A container running a small Python or Spark job that reads from a source, processes data, and writes to a destination, orchestrated by a workflow tool.\n",
      "\n",
      "- Model training service\n",
      "  - A container that trains a model on a dataset, logs metrics, and stores artifacts to a model registry or storage.\n",
      "\n",
      "Best practices\n",
      "\n",
      "- Keep images lean\n",
      "  - Use slim/base images, pin versions, and remove build-time secrets. Use multi-stage builds when you need build tools only for compilation.\n",
      "\n",
      "- Security and access\n",
      "  - Run processes as non-root where possible; avoid embedding credentials in images; use env vars or secret managers.\n",
      "\n",
      "- Reproducibility and dependencies\n",
      "  - Use a requirements.txt (or poetry) and system package lists that are well-scoped and pinned.\n",
      "\n",
      "- Versioning and documentation\n",
      "  - Tag images clearly (e.g., project-name:experiment-2025-06-01) and maintain a Dockerfile and a brief README describing the environment.\n",
      "\n",
      "- Data handling\n",
      "  - Use volumes or bind mounts for data to keep datasets external to the container and easily updatable.\n",
      "  - Be mindful of data provenance and lineage when containerizing data transformations.\n",
      "\n",
      "Common pitfalls to avoid\n",
      "\n",
      "- Large, bloated images\n",
      "  - Regularly prune unused layers and data; prefer multi-stage builds.\n",
      "\n",
      "- Data drift and hidden dependencies\n",
      "  - If data or external services change, rebuild and re-test images to catch drift early.\n",
      "\n",
      "- Over-reliance on container orchestration\n",
      "  - Start simple: containers on a single machine with docker-compose before moving to Kubernetes.\n",
      "\n",
      "- Not handling secrets securely\n",
      "  - Do not bake credentials into images; use environment variables, secret stores, or secret management in the cluster.\n",
      "\n",
      "Quick starter tips\n",
      "\n",
      "- Try official images for data science work (e.g., Jupyter stacks) as a quick start.\n",
      "- Use volumes for code and data to preserve work outside containers.\n",
      "- Use docker-compose to define related services (e.g., notebook, database, and a data service) in one go.\n",
      "- For GPU workloads, ensure the NVIDIA container toolkit is installed and the runtime is set to nvidia.\n",
      "\n",
      "If you’d like, tell me your use case (e.g., notebook-based exploration, ETL pipelines, model training, or deployment), and I can propose a concrete Dockerfile and a simple docker-compose setup tailored to your stack.\n"
     ]
    }
   ],
   "source": [
    "userPrompt = input(\"Ask whatever your heart desires: \")\n",
    "response = askChatBotQuestion(userPrompt)\n",
    "print(f\"{model} says: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
